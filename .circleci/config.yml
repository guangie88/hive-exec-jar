version: 2
jobs:
  build:
    docker:
    - image: maven:3-jdk-8-alpine
    environment:
      MAVEN_OPTS: "-Xmx2g"
      MAVEN_THREAD_COUNT: "1"
      GPG_REAL_NAME: Chen Weiguang
      GPG_EMAIL: chen.weiguang@gmail.com
      HIVE_REPO_URL: https://github.com/guangie88/hive.git
      HIVE_REPO_REF: release-1.2.1-spark2-allow-hadoop3
    steps:
    - run:
        name: Install git and gnupg
        command: |
          apk add --no-cache \
            git \
            gnupg
    - run:
        name: Get additional values for checking cache
        command: |
          echo "export UNAME_HASH=$(uname -a | md5sum | cut -d' ' -f1)" >> $BASH_ENV
          echo "export GPG_NAME_EMAIL_HASH=$(echo ${GPG_REAL_NAME}-${GPG_EMAIL} | md5sum | cut -d' ' -f1)" >> $BASH_ENV
          echo "export HIVE_REPO_SHA=$(git ls-remote --refs -q ${HIVE_REPO_URL} ${HIVE_REPO_REF} | cut -f1)" >> $BASH_ENV
          echo -e "${UNAME_HASH}\n${GPG_NAME_EMAIL_HASH}\n${HIVE_REPO_SHA}" > /tmp/cache-hashes

    - restore_cache:
        keys: v1-hive-exec-{{ checksum "/tmp/cache-hashes" }}
    - run:
        name: Create default gpg keypair and expose the public key
        command: |
          cat >specs <<EOF
            %echo Generating a default key
            Key-Type: default
            Subkey-Type: default
            Name-Real: ${GPG_REAL_NAME}
            Name-Email: ${GPG_EMAIL}
            Expire-Date: 0
            %no-protection
            %commit
            %echo done
          EOF
          gpg --batch --generate-key specs
          gpg -a --export "${GPG_REAL_NAME}"
    - run:
        name: Fetch repo with modified Hive for Spark that allows for Hadoop 3.y.z
        command: |
          git clone ${HIVE_REPO_URL} -b ${HIVE_REPO_REF} --depth 1
    - run:
        name: Perform Maven build (and prevent weird GPG problems)
        command: |
          echo "allow-loopback-pinentry" > ${HOME}/.gnupg/gpg-agent.conf
          echo "pinentry-mode loopback" > ${HOME}/.gnupg/gpg.conf
          cd hive
          mvn -T ${MAVEN_THREAD_COUNT} install -Phadoop-2 -DskipTests
    - run:
        name: Copy target JAR as artifact
        command: |
          mkdir -p /tmp/artifacts
          cp ./hive/ql/target/hive-exec-1.2.1.spark2.jar /tmp/artifacts/
    - save_cache:
        keys: v1-hive-exec-{{ checksum "/tmp/cache-hashes" }}
        paths:
        - /tmp/artifacts

    - persist_to_workspace:
        root: /tmp/artifacts
        paths:
        - hive-exec-1.2.1.spark2.jar
    
  publish:
    docker:
    - image: apihackers/ghr:0.10.2
    environment:
      RELEASE_TAG: 1.2.1.spark2-hadoop3
    steps:
    - attach_workspace:
        at: /tmp/artifacts
    - run:
        name: Publish target JAR for GitHub release
        command: |
          ghr -t ${GITHUB_TOKEN} -u ${CIRCLE_PROJECT_USERNAME} -r ${CIRCLE_PROJECT_REPONAME} -c ${CIRCLE_SHA1} -delete ${RELEASE_TAG} /tmp/artifacts/

# This assumes that "Only build pull requests" feature is turned OFF for the project
workflows:
  version: 2
  build_and_publish:
    jobs:
    - build
    - publish:
        requires:
          - build
        filters:
          branches:
            only:
            - master
